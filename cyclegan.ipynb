{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7908052,"sourceType":"datasetVersion","datasetId":4645441},{"sourceId":8710304,"sourceType":"datasetVersion","datasetId":5225128}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/def0017/style-transfer-using-cyclegan?scriptVersionId=200551211\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom PIL import Image\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nimport os\nfrom tqdm import tqdm\nfrom torchvision.utils import save_image","metadata":{"_uuid":"16a016a5-2978-4981-8c87-4b03dfff01e4","_cell_guid":"e84eea69-70f6-4784-a1a9-ee4c5ede5a2a","collapsed":false,"execution":{"iopub.status.busy":"2024-06-17T13:31:35.416284Z","iopub.execute_input":"2024-06-17T13:31:35.417329Z","iopub.status.idle":"2024-06-17T13:31:35.42339Z","shell.execute_reply.started":"2024-06-17T13:31:35.41729Z","shell.execute_reply":"2024-06-17T13:31:35.422334Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Discriminator Model","metadata":{"_uuid":"3dda3435-872a-426a-b2a5-f93bc400be59","_cell_guid":"43753522-9f30-4b51-9372-b4f1021cb324","trusted":true}},{"cell_type":"code","source":"class CNNBlock(nn.Module):\n  def __init__(self, in_channels, out_channels, stride, initial=False):\n    super().__init__()\n    self.conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=4, stride=stride, padding=1, bias=True, padding_mode=\"reflect\")\n    self.instanceNorm = nn.InstanceNorm2d(out_channels)\n    self.lrelu = nn.LeakyReLU(0.2)\n    self.initial = initial\n\n  def forward(self, x):\n    x = self.conv(x)\n    x = self.instanceNorm(x) if self.initial==False else x\n    x = self.lrelu(x)\n    return x\n\nclass Discriminator(nn.Module):\n  def __init__(self, in_channels=3, features=[64, 128, 256, 512]):\n    super().__init__()\n    layers=[]\n    layers.append(CNNBlock(in_channels, features[0], 2, initial=True))\n    in_channels = features[0]\n    for feature in features[1:]:\n      layers.append(CNNBlock(in_channels, feature, stride=1 if feature==features[-1] else 2))\n      in_channels = feature\n    layers.append(nn.Conv2d(in_channels=in_channels, out_channels=1, kernel_size=4, stride=1, padding=1, padding_mode=\"reflect\"))\n\n    self.model = nn.Sequential(*layers)\n\n  def forward(self, x):\n    return torch.sigmoid(self.model(x))","metadata":{"_uuid":"6d287dc3-f496-42f1-a001-87c18d5715eb","_cell_guid":"22db0438-1e5f-4c3e-9fbd-e6401d3473d1","collapsed":false,"execution":{"iopub.status.busy":"2024-06-17T13:31:35.425469Z","iopub.execute_input":"2024-06-17T13:31:35.425873Z","iopub.status.idle":"2024-06-17T13:31:35.437894Z","shell.execute_reply.started":"2024-06-17T13:31:35.425838Z","shell.execute_reply":"2024-06-17T13:31:35.437053Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Generator Model","metadata":{"_uuid":"26a62f4b-6ff6-4760-8073-1ee8e182e3bf","_cell_guid":"c2419002-74f5-43f7-b434-657205deba40","trusted":true}},{"cell_type":"code","source":"class cnnBlock(nn.Module):\n  def __init__(self, in_channels, out_channels, up_sample=False, use_act=True, **kwargs):\n    super().__init__()\n    self.cnn_block = nn.Sequential(\n        nn.ConvTranspose2d(in_channels=in_channels, out_channels=out_channels, **kwargs)\n        if up_sample else\n        nn.Conv2d(in_channels=in_channels, out_channels=out_channels, padding_mode=\"reflect\", **kwargs),\n        nn.InstanceNorm2d(out_channels),\n        nn.ReLU(inplace=True) if use_act else nn.Identity()\n    )\n\n  def forward(self, x):\n    return self.cnn_block(x)\n\nclass residualBlock(nn.Module):\n  def __init__(self, channels):\n    super().__init__()\n    self.resBlock = nn.Sequential(\n        cnnBlock(channels, channels, kernel_size=3, padding=1),\n        cnnBlock(channels, channels, use_act=False, kernel_size=3, padding=1)\n    )\n\n  def forward(self, x):\n    return x + self.resBlock(x)\n\nclass Generator(nn.Module):\n  def __init__(self, img_channels=3, features=64, num_residual=9):\n    super().__init__()\n    self.initial = nn.Sequential(\n        nn.Conv2d(img_channels, 64, kernel_size=7, stride=1, padding=3, padding_mode=\"reflect\"),\n        nn.ReLU()\n    )\n    self.downBlock = nn.ModuleList([\n        cnnBlock(features, features*2, kernel_size=3, stride=2, padding=1),\n        cnnBlock(features*2, features*4, kernel_size=3, stride=2, padding=1)\n    ])\n    self.resBlock = nn.Sequential(*[residualBlock(features*4) for _ in range(num_residual)])\n    self.upBlock = nn.ModuleList([\n        cnnBlock(features*4, features*2, up_sample=True, kernel_size=3, stride=2, padding=1, output_padding=1),\n        cnnBlock(features*2, features, up_sample=True, kernel_size=3, stride=2, padding=1, output_padding=1),\n    ])\n    self.final = nn.Conv2d(features, img_channels, kernel_size=7, stride=1, padding=3, padding_mode=\"reflect\")\n\n  def forward(self, x):\n    x = self.initial(x)\n    for layer in self.downBlock:\n      x = layer(x)\n    x = self.resBlock(x)\n    for layer in self.upBlock:\n      x = layer(x)\n    x = self.final(x)\n    return torch.tanh(x)","metadata":{"_uuid":"a53da7bd-fd28-493a-8fc0-294a828fb26d","_cell_guid":"315522a5-9c44-481b-ae92-c34f243d17f9","collapsed":false,"execution":{"iopub.status.busy":"2024-06-17T13:31:35.439072Z","iopub.execute_input":"2024-06-17T13:31:35.439339Z","iopub.status.idle":"2024-06-17T13:31:35.454495Z","shell.execute_reply.started":"2024-06-17T13:31:35.439316Z","shell.execute_reply":"2024-06-17T13:31:35.453656Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nTRAIN_DIR = \"/kaggle/input/vangogh2photo/vangogh2photo/train\"\nVAL_DIR = \"/kaggle/input/vangogh2photo/vangogh2photo/val\"\nBATCH_SIZE = 1\nLEARNING_RATE = 2e-4\nLAMBDA_IDENTITY = 0.0\nLAMBDA_CYCLE = 10\nNUM_WORKERS = 4\nNUM_EPOCHS = 0\nLOAD_MODEL = True\nSAVE_MODEL = True\nCHECKPOINT_GEN_B = \"/kaggle/input/checkpoints/genB.pth.tar\"\nCHECKPOINT_GEN_A = \"/kaggle/input/checkpoints/genA.pth.tar\"\nCHECKPOINT_DISC_A = \"/kaggle/input/checkpoints/discA.pth.tar\"\nCHECKPOINT_DISC_B = \"/kaggle/input/checkpoints/discB.pth.tar\"\n\ntransforms = A.Compose(\n    [\n        A.Resize(width=256, height=256),\n        A.HorizontalFlip(p=0.5),\n        A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], max_pixel_value=255),\n        ToTensorV2(),\n    ],\n    additional_targets={\"image0\": \"image\"},\n    is_check_shapes=False\n)","metadata":{"_uuid":"91de174d-5afe-4c3b-b64d-4ab294a15243","_cell_guid":"4e99ee94-191d-47d1-9e0f-b061d2862c98","collapsed":false,"execution":{"iopub.status.busy":"2024-06-17T13:31:35.455623Z","iopub.execute_input":"2024-06-17T13:31:35.45595Z","iopub.status.idle":"2024-06-17T13:31:35.468396Z","shell.execute_reply.started":"2024-06-17T13:31:35.45592Z","shell.execute_reply":"2024-06-17T13:31:35.467595Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ABDataset(Dataset):\n  def __init__(self, root_A, root_B, transform=None):\n    self.root_A = root_A\n    self.root_B = root_B\n    self.transform = transform\n    self.A_images = os.listdir(root_A)\n    self.B_images = os.listdir(root_B)\n    self.A_len = len(self.A_images)\n    self.B_len = len(self.B_images)\n    self.length_dataset = max(self.A_len, self.B_len) # 1000, 1500\n\n  def __len__(self):\n    return self.length_dataset\n\n  def __getitem__(self, index):\n    A_img = self.A_images[index % self.A_len]\n    B_img = self.B_images[index % self.B_len]\n\n    A_path = os.path.join(self.root_A, A_img)\n    B_path = os.path.join(self.root_B, B_img)\n\n    A_img = np.array(Image.open(A_path).convert(\"RGB\"))\n    B_img = np.array(Image.open(B_path).convert(\"RGB\"))\n\n    if self.transform:\n      augmentations = self.transform(image=A_img, image0=B_img)\n      A_img = augmentations[\"image\"]\n      B_img = augmentations[\"image0\"]\n\n    return A_img, B_img","metadata":{"_uuid":"5ca632ff-c21a-4982-8550-5a42353687f1","_cell_guid":"99e40242-7818-4461-9176-138075855a11","collapsed":false,"execution":{"iopub.status.busy":"2024-06-17T13:31:35.470044Z","iopub.execute_input":"2024-06-17T13:31:35.470335Z","iopub.status.idle":"2024-06-17T13:31:35.483619Z","shell.execute_reply.started":"2024-06-17T13:31:35.47031Z","shell.execute_reply":"2024-06-17T13:31:35.482657Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_fn(discA, discB, genB, genA, loader, optim_disc, optim_gen, l1, mse, d_scaler, g_scaler):\n  H_reals = 0\n  H_fakes = 0\n  loop = tqdm(loader, leave=True)\n\n  for idx, (imgA, imgB) in enumerate(loop):\n    imgA = imgA.to(DEVICE)\n    imgB = imgB.to(DEVICE)\n\n    # Train Discriminator\n    with torch.cuda.amp.autocast():\n      fake_imgA = genA(imgB)\n      dA_real = discA(imgA)\n      dA_fake = discA(fake_imgA.detach())\n\n      lossA_real = mse(dA_real, torch.ones_like(dA_real))\n      lossA_fake = mse(dA_fake, torch.zeros_like(dA_fake))\n      lossA = lossA_real + lossA_fake\n\n      fake_imgB = genB(imgA)\n      dB_real = discB(imgB)\n      dB_fake = discB(fake_imgB.detach())\n\n      lossB_real = mse(dB_real, torch.ones_like(dB_real))\n      lossB_fake = mse(dB_fake, torch.zeros_like(dB_fake))\n      lossB = lossB_real + lossB_fake\n\n      D_loss = (lossA+lossB)/2\n\n    optim_disc.zero_grad()\n    d_scaler.scale(D_loss).backward()\n    d_scaler.step(optim_disc)\n    d_scaler.update()\n\n    # Train Generator\n    with torch.cuda.amp.autocast():\n      # Adversarial Loss\n      dA_fake = discA(fake_imgA)\n      dB_fake = discB(fake_imgB)\n      lossA_fake = mse(dA_fake, torch.ones_like(dA_fake))\n      lossB_fake = mse(dB_fake, torch.ones_like(dB_fake))\n\n      # Cycle Loss\n      cycleB = genB(fake_imgA)\n      cycleA = genA(fake_imgB)\n      cycleA_loss = l1(imgA, cycleA)\n      cycleB_loss = l1(imgB, cycleB)\n\n      # Identity Loss\n      identityA = genA(imgA)\n      identityB = genB(imgB)\n      identityA_loss = l1(imgA, identityA)\n      identityB_loss = l1(imgB, identityB)\n\n      # total loss\n      G_loss = lossA_fake + lossB_fake + LAMBDA_CYCLE*(cycleA_loss + cycleB_loss) + LAMBDA_IDENTITY*(identityA_loss + identityB_loss)\n\n    optim_gen.zero_grad()\n    g_scaler.scale(G_loss).backward()\n    g_scaler.step(optim_gen)\n    g_scaler.update()\n\n    if idx % 200 == 0:\n      save_image(fake_imgA * 0.5 + 0.5, f\"/kaggle/working/A_{idx}.png\")\n      save_image(fake_imgB * 0.5 + 0.5, f\"/kaggle/working/B_{idx}.png\")\n\n    loop.set_postfix(H_real=H_reals / (idx + 1), H_fake=H_fakes / (idx + 1))","metadata":{"_uuid":"d14a452e-dcd5-408a-8c4b-9036c23667f7","_cell_guid":"ed0303d5-9dc0-4ade-8dca-43e9b3315fae","collapsed":false,"execution":{"iopub.status.busy":"2024-06-17T13:31:35.571491Z","iopub.execute_input":"2024-06-17T13:31:35.571775Z","iopub.status.idle":"2024-06-17T13:31:35.585501Z","shell.execute_reply.started":"2024-06-17T13:31:35.571753Z","shell.execute_reply":"2024-06-17T13:31:35.5845Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def save_checkpoint(model, optimizer, filename=\"my_checkpoint.pth.tar\"):\n    print(\"=> Saving checkpoint\")\n    checkpoint = {\n        \"state_dict\": model.state_dict(),\n        \"optimizer\": optimizer.state_dict(),\n    }\n    torch.save(checkpoint, filename)\n\n\ndef load_checkpoint(checkpoint_file, model, optimizer, lr):\n    print(\"=> Loading checkpoint\")\n    checkpoint = torch.load(checkpoint_file, map_location=DEVICE)\n    model.load_state_dict(checkpoint[\"state_dict\"])\n    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n\n    # If we don't do this then it will just have learning rate of old checkpoint\n    # and it will lead to many hours of debugging \\:\n    for param_group in optimizer.param_groups:\n        param_group[\"lr\"] = lr","metadata":{"_uuid":"de985768-cc4d-4ab8-8400-614786795390","_cell_guid":"94d9bd40-f9bf-4bc0-bae8-843d343d9517","collapsed":false,"execution":{"iopub.status.busy":"2024-06-17T13:31:35.587397Z","iopub.execute_input":"2024-06-17T13:31:35.587672Z","iopub.status.idle":"2024-06-17T13:31:35.599232Z","shell.execute_reply.started":"2024-06-17T13:31:35.587649Z","shell.execute_reply":"2024-06-17T13:31:35.598374Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"discA = Discriminator().to(DEVICE)\ndiscB = Discriminator().to(DEVICE)\ngenB = Generator().to(DEVICE)\ngenA = Generator().to(DEVICE)\ndef main():\n  optim_disc = optim.Adam(list(discA.parameters()) + list(discB.parameters()), lr=LEARNING_RATE, betas=(0.5, 0.999))\n  optim_gen = optim.Adam(list(genB.parameters()) + list(genA.parameters()), lr=LEARNING_RATE, betas=(0.5, 0.999))\n  l1 = nn.L1Loss()\n  mse = nn.MSELoss()\n\n  if LOAD_MODEL:\n    load_checkpoint(CHECKPOINT_GEN_A, genA, optim_gen, LEARNING_RATE)\n    load_checkpoint(CHECKPOINT_GEN_B, genB, optim_gen, LEARNING_RATE)\n    load_checkpoint(CHECKPOINT_DISC_A, discA, optim_disc, LEARNING_RATE)\n    load_checkpoint(CHECKPOINT_DISC_B, discB, optim_disc, LEARNING_RATE)\n\n  dataset = ABDataset(root_A=TRAIN_DIR + \"/trainA\", root_B=TRAIN_DIR + \"/trainB\", transform=transforms)\n  val_dataset = ABDataset(root_A=VAL_DIR + \"/testA\", root_B=VAL_DIR + \"/testB\", transform=transforms)\n  val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, pin_memory=True)\n  loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n  g_scaler = torch.cuda.amp.GradScaler()\n  d_scaler = torch.cuda.amp.GradScaler()\n\n  for epoch in range(NUM_EPOCHS):\n    print(\"Epoch: \", epoch)\n    train_fn(discA, discB, genB, genA, loader, optim_disc, optim_gen, l1, mse, d_scaler, g_scaler)\n\n    if SAVE_MODEL:\n      save_checkpoint(genA, optim_gen, filename=\"genA.pth.tar\")\n      save_checkpoint(genB, optim_gen, filename=\"genB.pth.tar\")\n      save_checkpoint(discA, optim_disc, filename=\"discA.pth.tar\")\n      save_checkpoint(discB, optim_disc, filename=\"discA.pth.tar\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"_uuid":"57e1b6a3-9405-4788-8aff-1e883eaea323","_cell_guid":"d19f3e82-fb4a-403d-ae7a-ff116985c810","collapsed":false,"execution":{"iopub.status.busy":"2024-06-17T13:31:35.600196Z","iopub.execute_input":"2024-06-17T13:31:35.600496Z","iopub.status.idle":"2024-06-17T14:26:38.166625Z","shell.execute_reply.started":"2024-06-17T13:31:35.600465Z","shell.execute_reply":"2024-06-17T14:26:38.164993Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport cv2\n\ndir = []\nfigure, axis = plt.subplots(3, 4)\nfigure.tight_layout(pad=0.13,rect=[0, 0.03, 1, 0.91])\nfor x in os.listdir(\"/kaggle/working/\"):\n    split_x = os.path.splitext(x)\n    if split_x[-1] == \".png\":\n        dir.append(x)\n\nfor ax, x in zip(axis.flat, dir):\n    split_x = os.path.splitext(x)\n    img = cv2.imread(\"/kaggle/working/\" + x)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    ax.imshow(img)\n    ax.set_title(split_x[0])\n    ax.set_axis_off()\nplt.show()","metadata":{"_uuid":"9050454a-c3b7-4c88-a05d-3ab496a8d10f","_cell_guid":"c790021a-b05e-4591-ae6a-c547575035ff","collapsed":false,"execution":{"iopub.status.busy":"2024-06-17T14:26:38.168583Z","iopub.execute_input":"2024-06-17T14:26:38.16902Z","iopub.status.idle":"2024-06-17T14:26:39.608208Z","shell.execute_reply.started":"2024-06-17T14:26:38.168986Z","shell.execute_reply":"2024-06-17T14:26:39.607324Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transforms2 = A.Compose(\n    [\n        A.Resize(width=256, height=256),\n        A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], max_pixel_value=255),\n        ToTensorV2(),\n    ],\n    is_check_shapes=False\n)\ndef style_transfer(img_file):\n    img = np.array(Image.open(img_file))\n    transform_img = transforms2(image=img)\n    input_img = transform_img[\"image\"]\n    input_img = input_img.to(DEVICE)\n    output_img = genA(input_img)\n    save_image(output_img*0.5 + 0.5, f\"/kaggle/working/output.png\")","metadata":{"_uuid":"4683b524-0b22-466f-add8-843820858694","_cell_guid":"5e9d92a2-3dc6-4085-b570-73c86ffe9b2b","collapsed":false,"execution":{"iopub.status.busy":"2024-06-17T14:36:42.728516Z","iopub.execute_input":"2024-06-17T14:36:42.729179Z","iopub.status.idle":"2024-06-17T14:36:42.735845Z","shell.execute_reply.started":"2024-06-17T14:36:42.729146Z","shell.execute_reply":"2024-06-17T14:36:42.734835Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_file = \"/kaggle/input/vangogh2photo/vangogh2photo/val/testB/739.jpg\"\nstyle_transfer(img_file)","metadata":{"_uuid":"62ed2381-1f98-4f89-8306-d99d0759e359","_cell_guid":"cff2b0d1-6c17-4682-bdf2-43d3500d93f8","collapsed":false,"execution":{"iopub.status.busy":"2024-06-17T14:41:55.012037Z","iopub.execute_input":"2024-06-17T14:41:55.012825Z","iopub.status.idle":"2024-06-17T14:41:55.08376Z","shell.execute_reply.started":"2024-06-17T14:41:55.012794Z","shell.execute_reply":"2024-06-17T14:41:55.082739Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output_img = cv2.imread('/kaggle/working/output.png')\ninput_img = cv2.imread('/kaggle/input/vangogh2photo/vangogh2photo/val/testB/739.jpg')\noutput_img = cv2.cvtColor(output_img, cv2.COLOR_BGR2RGB)\ninput_img = cv2.cvtColor(input_img, cv2.COLOR_BGR2RGB)\nfig, axes = plt.subplots(1, 2, figsize=(10, 5))\n\n# Display the first image in the first subplot\naxes[0].imshow(input_img)\naxes[0].set_title('Input Image')\naxes[0].axis('off')  # Hide axes\n\n# Display the second image in the second subplot\naxes[1].imshow(output_img)\naxes[1].set_title('Output Image')\naxes[1].axis('off')  # Hide axes\n\n# Show the plot\nplt.show()","metadata":{"_uuid":"08ff8f58-ef67-42b8-baba-4bd2f61ca18e","_cell_guid":"f8488b5b-1447-4088-8b33-ffea9b6d7376","collapsed":false,"execution":{"iopub.status.busy":"2024-06-17T14:42:16.201973Z","iopub.execute_input":"2024-06-17T14:42:16.202847Z","iopub.status.idle":"2024-06-17T14:42:16.744976Z","shell.execute_reply.started":"2024-06-17T14:42:16.202812Z","shell.execute_reply":"2024-06-17T14:42:16.743943Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_input = gr.inputs.Image(shape=(256, 256))\nimage_output = gr.outputs.Image(type=\"pil\")\n\ngr.Interface(fn=style_transfer, inputs=image_input, outputs=image_output, title=\"Style Transfer with CycleGAN\").launch()","metadata":{"_uuid":"80d3abb7-e8c1-42db-8c57-e30689d964af","_cell_guid":"d417579e-46cc-4738-a815-114137e78f80","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}